# Speech-to-Text Guide
## Local Speech Recognition with Whisper AI

This guide shows you how to add GPU-powered speech recognition using OpenAI's Whisper model.

---

## üéØ What We're Building

### Features:
- ‚úÖ Local speech-to-text processing
- ‚úÖ GPU-accelerated transcription
- ‚úÖ Multiple language support
- ‚úÖ High accuracy
- ‚úÖ Works offline

### Demo Flow:
```
User speaks ‚Üí Audio recorded ‚Üí Whisper (GPU) ‚Üí Text transcription ‚Üí AI Response
```

---

## üìä Architecture

### Current (Browser Speech API):
```
Microphone ‚Üí Browser API ‚Üí Text ‚Üí Backend
   ‚Üì
Limited accuracy, online only
```

### New (Whisper AI):
```
Microphone ‚Üí Audio file ‚Üí Backend ‚Üí Whisper (GPU) ‚Üí Text ‚Üí AI
   ‚Üì
High accuracy, works offline, GPU-powered
```

---

## Step 1: Create Audio Service (20 minutes)

Create a new service for audio processing:

```bash
cd backend/services
nano audio_service.py
```

Add this code:

```python
import whisper
import torch
import numpy as np
import io
import base64
from typing import Dict, Any, Optional
from pydub import AudioSegment

class AudioService:
    """
    Audio processing service using Whisper AI
    Provides GPU-accelerated speech-to-text
    """
    
    def __init__(self, model_size: str = "base"):
        """
        Initialize Whisper model
        
        Args:
            model_size: "tiny", "base", "small", "medium", "large"
                       Larger = more accurate but slower
        """
        self.model_size = model_size
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"Loading Whisper {model_size} model on {self.device}...")
        self.model = whisper.load_model(model_size, device=self.device)
        print(f"‚úì Whisper {model_size} model loaded")
        
        # Supported languages (Whisper supports 99 languages)
        self.supported_languages = [
            "en", "es", "fr", "de", "it", "pt", "nl", "pl", "ru", "ja", "ko", "zh"
        ]
    
    def transcribe_audio(
        self,
        audio_data: str,
        language: Optional[str] = None,
        task: str = "transcribe"
    ) -> Dict[str, Any]:
        """
        Transcribe audio to text
        
        Args:
            audio_data: Base64 encoded audio (WAV, MP3, etc.)
            language: Language code (e.g., "en", "es") or None for auto-detect
            task: "transcribe" or "translate" (translate to English)
        
        Returns:
            Dict with transcription and metadata
        """
        try:
            # Decode audio
            audio_array = self._decode_audio(audio_data)
            
            # Transcribe with Whisper
            options = {
                "task": task,
                "fp16": torch.cuda.is_available()  # Use FP16 on GPU for speed
            }
            
            if language:
                options["language"] = language
            
            result = self.model.transcribe(audio_array, **options)
            
            return {
                "success": True,
                "text": result["text"].strip(),
                "language": result.get("language", "unknown"),
                "segments": result.get("segments", []),
                "confidence": self._calculate_confidence(result),
                "model": self.model_size,
                "device": self.device
            }
            
        except Exception as e:
            print(f"Error transcribing audio: {e}")
            return {
                "success": False,
                "error": str(e),
                "text": ""
            }
    
    def _decode_audio(self, audio_data: str) -> np.ndarray:
        """
        Decode base64 audio to numpy array
        Whisper expects 16kHz mono audio
        """
        # Remove data URL prefix if present
        if "base64," in audio_data:
            audio_data = audio_data.split("base64,")[1]
        
        # Decode base64
        audio_bytes = base64.b64decode(audio_data)
        
        # Load audio with pydub
        audio = AudioSegment.from_file(io.BytesIO(audio_bytes))
        
        # Convert to mono
        if audio.channels > 1:
            audio = audio.set_channels(1)
        
        # Convert to 16kHz (Whisper's expected sample rate)
        audio = audio.set_frame_rate(16000)
        
        # Convert to numpy array
        samples = np.array(audio.get_array_of_samples())
        
        # Normalize to [-1, 1]
        samples = samples.astype(np.float32) / 32768.0
        
        return samples
    
    def _calculate_confidence(self, result: Dict) -> float:
        """Calculate average confidence from segments"""
        segments = result.get("segments", [])
        if not segments:
            return 0.0
        
        # Whisper doesn't provide confidence directly
        # We can estimate from no_speech_prob
        confidences = []
        for segment in segments:
            no_speech_prob = segment.get("no_speech_prob", 0.5)
            confidence = 1.0 - no_speech_prob
            confidences.append(confidence)
        
        return sum(confidences) / len(confidences) if confidences else 0.0
    
    def detect_language(self, audio_data: str) -> Dict[str, Any]:
        """
        Detect the language of audio
        
        Args:
            audio_data: Base64 encoded audio
        
        Returns:
            Dict with detected language and confidence
        """
        try:
            audio_array = self._decode_audio(audio_data)
            
            # Detect language (first 30 seconds)
            audio_sample = audio_array[:30 * 16000]
            audio_tensor = torch.from_numpy(audio_sample).to(self.device)
            
            # Pad or trim to 30 seconds
            audio_tensor = whisper.pad_or_trim(audio_tensor)
            
            # Make log-Mel spectrogram
            mel = whisper.log_mel_spectrogram(audio_tensor).to(self.device)
            
            # Detect language
            _, probs = self.model.detect_language(mel)
            
            # Get top 3 languages
            top_langs = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
            
            return {
                "success": True,
                "detected_language": top_langs[0][0],
                "confidence": top_langs[0][1],
                "top_languages": [
                    {"language": lang, "confidence": conf}
                    for lang, conf in top_langs
                ]
            }
            
        except Exception as e:
            print(f"Error detecting language: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about loaded model"""
        return {
            "model_size": self.model_size,
            "device": self.device,
            "gpu_available": torch.cuda.is_available(),
            "supported_languages": self.supported_languages,
            "model_parameters": sum(p.numel() for p in self.model.parameters())
        }
    
    def switch_model(self, model_size: str) -> bool:
        """
        Switch to a different Whisper model
        
        Args:
            model_size: "tiny", "base", "small", "medium", "large"
        
        Returns:
            True if successful
        """
        try:
            print(f"Switching to Whisper {model_size} model...")
            self.model = whisper.load_model(model_size, device=self.device)
            self.model_size = model_size
            print(f"‚úì Switched to {model_size} model")
            return True
        except Exception as e:
            print(f"‚úó Failed to switch model: {e}")
            return False
```

Save and exit.

---

## Step 2: Install Audio Dependencies (5 minutes)

```bash
cd ../..
nano requirements.txt
```

Add:
```
openai-whisper==20231117
pydub==0.25.1
ffmpeg-python==0.2.0
```

Install:
```bash
pip install openai-whisper pydub ffmpeg-python
```

Install ffmpeg (required by pydub):
```bash
# On Ubuntu/Debian
sudo apt install ffmpeg -y

# Verify installation
ffmpeg -version
```

---

## Step 3: Add Audio Endpoints to API (15 minutes)

Update `backend/main.py`:

```bash
nano backend/main.py
```

Add imports:
```python
from services.audio_service import AudioService
```

Initialize service:
```python
# Add after other service initializations
audio_service = AudioService(model_size="base")  # or "small" for better accuracy
```

Add request models:
```python
class TranscribeRequest(BaseModel):
    audio: str  # Base64 encoded audio
    language: Optional[str] = None
    session_id: Optional[str] = None

class DetectLanguageRequest(BaseModel):
    audio: str  # Base64 encoded audio
```

Add endpoints:
```python
@app.post("/audio/transcribe")
async def transcribe_audio(request: TranscribeRequest, current_user: dict = Depends(get_current_user)):
    """Transcribe audio to text using Whisper"""
    result = audio_service.transcribe_audio(
        audio_data=request.audio,
        language=request.language
    )
    
    return result

@app.post("/audio/detect-language")
async def detect_language(request: DetectLanguageRequest):
    """Detect the language of audio"""
    result = audio_service.detect_language(request.audio)
    return result

@app.get("/audio/model-info")
async def get_audio_model_info():
    """Get information about the Whisper model"""
    return audio_service.get_model_info()

@app.post("/audio/switch-model")
async def switch_audio_model(model_size: str):
    """Switch to a different Whisper model"""
    success = audio_service.switch_model(model_size)
    return {
        "success": success,
        "model_info": audio_service.get_model_info()
    }

@app.post("/audio/speech-to-response")
async def speech_to_response(request: TranscribeRequest, current_user: dict = Depends(get_current_user)):
    """
    Complete flow: Speech ‚Üí Text ‚Üí AI Response
    """
    # Transcribe audio
    transcription = audio_service.transcribe_audio(
        audio_data=request.audio,
        language=request.language
    )
    
    if not transcription["success"]:
        return {
            "success": False,
            "error": transcription.get("error", "Transcription failed")
        }
    
    text = transcription["text"]
    
    if not text:
        return {
            "success": False,
            "error": "No speech detected"
        }
    
    # Process through communication pipeline
    comm_result = await coordinator.process_communication(
        input_text=text,
        user_type="verbal",  # Speech is verbal communication
        session_id=request.session_id
    )
    
    return {
        "success": True,
        "transcription": transcription,
        "text": text,
        "language": transcription.get("language"),
        "confidence": transcription.get("confidence"),
        "ai_response": comm_result.get("output", {}).get("text", "")
    }
```

---

## Step 4: Update Frontend - Add Audio Recording (25 minutes)

Update `frontend/dashboard.html`:

```bash
nano frontend/dashboard.html
```

Add audio recording section:
```html
<!-- Add after webcam section -->
<div class="audio-section">
    <h3>üé§ Speech Recognition</h3>
    <div class="audio-controls">
        <button onclick="startRecording()" id="startRecordBtn">üé§ Start Recording</button>
        <button onclick="stopRecording()" id="stopRecordBtn" style="display: none;">‚èπÔ∏è Stop Recording</button>
        <div class="recording-indicator" id="recordingIndicator" style="display: none;">
            üî¥ Recording...
        </div>
    </div>
    <div class="audio-info">
        <p>Click to record your voice, then stop to transcribe and get AI response</p>
        <p id="transcriptionResult"></p>
    </div>
</div>
```

Add CSS:
```html
<style>
.audio-section {
    margin: 20px 0;
    padding: 20px;
    background: white;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.audio-controls {
    text-align: center;
    margin: 15px 0;
}

.audio-controls button {
    margin: 0 10px;
    padding: 15px 30px;
    font-size: 18px;
    cursor: pointer;
    border: none;
    border-radius: 8px;
    background: #2196F3;
    color: white;
}

.audio-controls button:hover {
    background: #1976D2;
}

.recording-indicator {
    display: inline-block;
    margin-left: 15px;
    padding: 10px 20px;
    background: #f44336;
    color: white;
    border-radius: 4px;
    font-weight: bold;
    animation: pulse 1.5s infinite;
}

@keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
}

.audio-info {
    text-align: center;
    color: #666;
    margin-top: 15px;
}

#transcriptionResult {
    margin-top: 10px;
    padding: 10px;
    background: #f5f5f5;
    border-radius: 4px;
    font-style: italic;
}
</style>
```

---

## Step 5: Add Audio JavaScript (20 minutes)

Update `frontend/app.js`:

```bash
nano frontend/app.js
```

Add audio recording functionality:

```javascript
// Audio recording variables
let mediaRecorder = null;
let audioChunks = [];
let isRecording = false;

// Start audio recording
async function startRecording() {
    try {
        // Request microphone access
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // Create MediaRecorder
        mediaRecorder = new MediaRecorder(stream);
        
        // Collect audio data
        audioChunks = [];
        mediaRecorder.ondataavailable = (event) => {
            audioChunks.push(event.data);
        };
        
        // Handle recording stop
        mediaRecorder.onstop = async () => {
            await processRecording();
        };
        
        // Start recording
        mediaRecorder.start();
        isRecording = true;
        
        // Update UI
        document.getElementById('startRecordBtn').style.display = 'none';
        document.getElementById('stopRecordBtn').style.display = 'inline-block';
        document.getElementById('recordingIndicator').style.display = 'inline-block';
        
        console.log('Recording started');
        
    } catch (error) {
        console.error('Error accessing microphone:', error);
        alert('Could not access microphone. Please check permissions.');
    }
}

// Stop audio recording
function stopRecording() {
    if (mediaRecorder && isRecording) {
        mediaRecorder.stop();
        isRecording = false;
        
        // Stop all tracks
        mediaRecorder.stream.getTracks().forEach(track => track.stop());
        
        // Update UI
        document.getElementById('startRecordBtn').style.display = 'inline-block';
        document.getElementById('stopRecordBtn').style.display = 'none';
        document.getElementById('recordingIndicator').style.display = 'none';
        
        console.log('Recording stopped');
    }
}

// Process recorded audio
async function processRecording() {
    try {
        // Create audio blob
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        
        // Convert to base64
        const base64Audio = await blobToBase64(audioBlob);
        
        // Show loading
        document.getElementById('transcriptionResult').textContent = 'Transcribing...';
        addMessage('Processing speech...', 'system');
        
        // Send to backend
        const response = await fetch(`${API_BASE_URL}/audio/speech-to-response`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${localStorage.getItem('token')}`
            },
            body: JSON.stringify({
                audio: base64Audio,
                language: 'en',  // or auto-detect
                session_id: currentSessionId
            })
        });
        
        const result = await response.json();
        
        if (result.success) {
            // Display transcription
            document.getElementById('transcriptionResult').textContent = 
                `You said: "${result.text}"`;
            
            // Display in chat
            addMessage(result.text, 'user');
            addMessage(result.ai_response, 'assistant');
            
            console.log('Transcription:', result.text);
            console.log('Confidence:', result.confidence);
            console.log('Language:', result.language);
        } else {
            document.getElementById('transcriptionResult').textContent = 
                'Error: ' + (result.error || 'Transcription failed');
            addMessage('Could not transcribe speech', 'system');
        }
        
    } catch (error) {
        console.error('Error processing recording:', error);
        document.getElementById('transcriptionResult').textContent = 
            'Error processing audio';
        addMessage('Error processing speech', 'system');
    }
}

// Convert blob to base64
function blobToBase64(blob) {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onloadend = () => resolve(reader.result);
        reader.onerror = reject;
        reader.readAsDataURL(blob);
    });
}

// Check audio model status
async function checkAudioModelStatus() {
    try {
        const response = await fetch(`${API_BASE_URL}/audio/model-info`);
        const data = await response.json();
        
        console.log('Audio Model Info:', data);
        console.log(`Using Whisper ${data.model_size} on ${data.device}`);
        
    } catch (error) {
        console.error('Failed to check audio model status:', error);
    }
}

// Call on page load
checkAudioModelStatus();
```

---

## Step 6: Test Speech Recognition (15 minutes)

### Test 1: Check Model Info

```bash
curl http://localhost:8000/audio/model-info
```

Expected output:
```json
{
  "model_size": "base",
  "device": "cuda",
  "gpu_available": true,
  "supported_languages": ["en", "es", "fr", ...],
  "model_parameters": 74000000
}
```

### Test 2: Test with Sample Audio

Create a test script:
```bash
nano test_audio.py
```

```python
import sys
import base64
sys.path.append('backend')
from services.audio_service import AudioService

# Initialize service
audio_service = AudioService(model_size="base")

print("=== Audio Service Test ===")
print(f"Model Info: {audio_service.get_model_info()}\n")

# You would need a sample audio file to test
# For now, just verify initialization
print("‚úÖ Audio service initialized successfully!")
```

Run:
```bash
python test_audio.py
```

### Test 3: Test via Frontend

1. Start backend:
```bash
cd backend
python main.py
```

2. Open frontend in browser
3. Click "Start Recording"
4. Speak clearly: "Hello, how are you?"
5. Click "Stop Recording"
6. Wait for transcription and AI response

---

## üéØ Model Comparison

### Whisper Model Sizes:

| Model | Parameters | VRAM | Speed | Accuracy | Best For |
|-------|-----------|------|-------|----------|----------|
| tiny | 39M | ~1GB | Very Fast | Good | Testing, demos |
| base | 74M | ~1GB | Fast | Very Good | **Recommended** |
| small | 244M | ~2GB | Medium | Excellent | Production |
| medium | 769M | ~5GB | Slow | Excellent | High accuracy needed |
| large | 1550M | ~10GB | Very Slow | Best | Maximum accuracy |

**Recommendation:** Use "base" for development, "small" for production.

---

## üí° Advanced Features

### 1. Real-time Streaming

For continuous transcription:
```python
def transcribe_stream(self, audio_stream):
    """Transcribe audio in real-time"""
    # Implement streaming transcription
    pass
```

### 2. Speaker Diarization

Identify different speakers:
```python
# Requires additional library like pyannote.audio
from pyannote.audio import Pipeline
```

### 3. Noise Reduction

Pre-process audio to remove noise:
```python
import noisereduce as nr

def reduce_noise(audio_array, sample_rate):
    return nr.reduce_noise(y=audio_array, sr=sample_rate)
```

### 4. Multi-language Support

Auto-detect and transcribe any language:
```python
result = audio_service.transcribe_audio(
    audio_data=audio,
    language=None  # Auto-detect
)
```

---

## üêõ Troubleshooting

### Microphone Not Accessible

```javascript
// Check browser permissions
// Chrome: Settings ‚Üí Privacy ‚Üí Microphone
// Firefox: Preferences ‚Üí Privacy ‚Üí Permissions ‚Üí Microphone
```

### Transcription Errors

```python
# Try different model size
audio_service.switch_model("small")  # More accurate

# Or specify language explicitly
result = audio_service.transcribe_audio(
    audio_data=audio,
    language="en"  # Force English
)
```

### Slow Transcription

```python
# Use smaller model
audio_service = AudioService(model_size="tiny")

# Or enable FP16 (already enabled if GPU available)
# Check if GPU is being used
nvidia-smi
```

### Audio Format Issues

```bash
# Install additional codecs
sudo apt install libavcodec-extra -y

# Verify ffmpeg
ffmpeg -formats
```

---

## ‚úÖ Verification Checklist

- [ ] Whisper model loaded
- [ ] GPU being used (check nvidia-smi)
- [ ] Microphone accessible
- [ ] Recording works
- [ ] Transcription accurate
- [ ] AI responds correctly
- [ ] Multiple languages work
- [ ] Performance acceptable

---

## üéâ Complete!

You've successfully added GPU-powered speech recognition!

### What You Achieved:
- ‚úÖ Local speech-to-text with Whisper
- ‚úÖ GPU-accelerated transcription
- ‚úÖ Multi-language support
- ‚úÖ High accuracy recognition
- ‚úÖ Complete audio pipeline

### Your System Now Has:
1. ‚úÖ Local LLM (Llama 3)
2. ‚úÖ Computer Vision (MediaPipe)
3. ‚úÖ Speech Recognition (Whisper)
4. ‚úÖ Multi-modal AI system

---

## üìä Performance Metrics

### Expected Performance:

**Whisper Base Model on T4 GPU:**
- Transcription speed: 5-10x real-time
- 10 seconds of audio ‚Üí 1-2 seconds to transcribe
- Accuracy: 95%+ for clear speech
- Languages: 99 supported

**Comparison to Browser API:**
- Accuracy: 30-50% better
- Privacy: 100% local
- Offline: Works without internet
- Languages: 99 vs ~10

---

## üöÄ Next Steps

You now have a complete GPU-powered multi-modal AI system!

### Ready for Deployment:
1. All GPU features implemented
2. Local AI models running
3. Computer vision working
4. Speech recognition active

### For Hackathon Demo:
1. Test all features together
2. Prepare demo script
3. Record backup video
4. Practice presentation

**Your Communication Bridge AI is now fully GPU-enhanced!** üé§üé•ü§ñ
